{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "Let ${\\mathbf{x}}$ be an $n$-dimensional vector, the following rules are often used when differentiating functions:\n",
    "\n",
    "- For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\triangledown_{x}\\mathbf{Ax} = \\mathbf{A}^{\\top}$\n",
    "- For all $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, $\\triangledown_{x}\\mathbf{x^{\\top}A} = \\mathbf{A}$\n",
    "- For all $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\triangledown_{x}\\mathbf{x^{\\top}Ax} = (\\mathbf{A} + \\mathbf{A}^{\\top})\\mathbf{x}$\n",
    "- $\\triangledown_{x}||\\mathbf{x}||^2 = \\triangledown_{x}\\mathbf{x}^{\\top}\\mathbf{x} = 2\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "## Backward for Non-Scalar Variables\n",
    "\n",
    "从技术上而言，当 $y$ 不是一个标量而是一个向量(m-dimensional) 对向量 $x$(n-dimensional)求导，求导结果是一个称之为 Jacobian($m \\times n$ matrix)的矩阵。\n",
    "\n",
    "> 在向量求导中，这属于向量对向量求导。\n",
    "\n",
    "在深度学习（机器学习）中，通常对向量求导的作用是最小化损失函数，其核心是计算在一个批次中每个样本的偏导数之和，因此在 MXNet 框架中，对于向量对向量求导会按照向量求和的方式求导，也就是将向量对向量求导变为标量对向量求导。\n",
    "\n",
    "下面的例子就说明了 MXNet 的这种处理方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T03:11:19.364337Z",
     "start_time": "2019-12-05T03:11:16.930537Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd, np, npx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T03:11:19.383337Z",
     "start_time": "2019-12-05T03:11:19.372337Z"
    }
   },
   "outputs": [],
   "source": [
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T03:11:19.428337Z",
     "start_time": "2019-12-05T03:11:19.394337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector x\n",
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T03:12:21.791737Z",
     "start_time": "2019-12-05T03:12:21.760537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 4., 9.]), array([0., 2., 4., 6.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量对向量求导\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = x * x\n",
    "y.backward()\n",
    "y, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T03:12:39.310137Z",
     "start_time": "2019-12-05T03:12:39.274537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(14.),\n",
       " array([0., 2., 4., 6.]),\n",
       " array([0., 2., 4., 6.]),\n",
       " array([1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量对向量求导\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "with autograd.record():\n",
    "    v = (u * u).sum()  # v is a scalar\n",
    "v.backward()\n",
    "v, u.grad, x.grad, x.grad == u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaching Computation\n",
    "\n",
    "分离计算，有时候我们希望将一些计算从计算图中移除。这句话不好理解，用例子解释一下，假设我们有 $y=f(x)=x^2$ 和 $z=f(x,y)=xy=x^3$，求 $\\frac{\\partial{z}}{\\partial{x}}$，具体求导算法如下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T08:59:37.480459Z",
     "start_time": "2019-12-05T08:59:37.433659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  3., 12., 27.]), array([0., 1., 2., 3.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = x * x\n",
    "    z = y * x\n",
    "z.backward()\n",
    "x.grad, u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里详细解释一下背后的数学计算过程：\n",
    "\n",
    "向量 $\\vec{x}=\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, 可以使用 $\\vec{x}=\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}$ 表示。 $z=yx=x^3=\\begin{bmatrix} x_1^3 \\\\ x_2^3 \\\\ x_3^3 \\\\ x_4^3 \\end{bmatrix}$，根据[上一章节](#Backward-for-Non-Scalar-Variables)的说明，对于结果是向量（这里就是$z$的计算结果是一个向量），MXNET 框架会将其转换为向量求和后来求导，也就是 $z=x_1^3+x_2^3+x_3^3+x_4^3$，此时就是$z$结果是一个标量，使用标量对向量求导的规则，可以得到：\n",
    "\n",
    "$\n",
    "\\frac{\\partial{z}}{\\partial{x}}=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial{(x_1^3+x_2^3+x_3^3+x_4^3)}}{\\partial{x_1}} \\\\\n",
    "    \\frac{\\partial{(x_1^3+x_2^3+x_3^3+x_4^3)}}{\\partial{x_2}} \\\\\n",
    "    \\frac{\\partial{(x_1^3+x_2^3+x_3^3+x_4^3)}}{\\partial{x_3}} \\\\\n",
    "    \\frac{\\partial{(x_1^3+x_2^3+x_3^3+x_4^3)}}{\\partial{x_4}} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    3x_1^2 \\\\ 3x_1^2 \\\\ 3x_1^2 \\\\ 3x_1^2\n",
    "\\end{bmatrix}\n",
    "= 3\\vec{x}\\vec{x}\n",
    "= 3\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0 \\\\ 3 \\\\ 12 \\\\ 27 \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于某种原因，在求导过程中我们想让 $y$ 被认为是一个常量，此时可以使用下面的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:05:01.121792Z",
     "start_time": "2019-12-03T09:05:01.108792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 4., 9.]), array([0., 1., 4., 9.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "z.backward()\n",
    "x.grad, u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
